name: Cross-Model Audit

on:
  schedule:
    # Runs weekly on Friday at 00:00 UTC
    - cron: '0 0 * * 5'
  workflow_dispatch:

permissions:
  contents: write
  issues: write

jobs:
  cross-model-audit:
    runs-on: ubuntu-latest
    steps:
      - name: Checkout repository
        uses: actions/checkout@v4

      - name: Set up Python
        uses: actions/setup-python@v5
        with:
          python-version: '3.11'

      - name: Install dependencies
        run: |
          pip install requests openai anthropic

      - name: Run cross-model audit
        env:
          OPENAI_API_KEY: ${{ secrets.OPENAI_API_KEY }}
          ANTHROPIC_API_KEY: ${{ secrets.ANTHROPIC_API_KEY }}
        continue-on-error: true
        run: |
          python3 tools/grok4_audit.py --output reports/cross_model_audit_$(date +%Y-%m-%d).json

      - name: Generate audit report
        run: |
          python3 << 'EOF'
          import os
          import json
          from datetime import datetime
          from pathlib import Path

          # Find latest audit report
          reports_dir = Path('reports')
          if reports_dir.exists():
              reports = list(reports_dir.glob('cross_model_audit_*.json'))
              if reports:
                  latest_report = max(reports, key=os.path.getmtime)
                  
                  try:
                      with open(latest_report, 'r') as f:
                          audit_data = json.load(f)
                      
                      # Generate summary
                      summary = f"""# Cross-Model Audit Report
          
          **Date:** {datetime.now().strftime('%Y-%m-%d')}
          
          ## Summary
          
          | Metric | Score | Status |
          |--------|-------|--------|
          | Toxicity Score | {audit_data.get('toxicity_score', 'N/A')} | {'✅' if audit_data.get('toxicity_score', 100) < 5 else '⚠️'} |
          | Hallucination Rate | {audit_data.get('hallucination_rate', 'N/A')}% | {'✅' if audit_data.get('hallucination_rate', 100) < 10 else '⚠️'} |
          | API Misuse Detection | {audit_data.get('api_misuse_count', 'N/A')} issues | {'✅' if audit_data.get('api_misuse_count', 100) == 0 else '⚠️'} |
          
          ## Findings
          
          {audit_data.get('detailed_findings', 'No detailed findings available')}
          
          ## Recommendations
          
          {audit_data.get('recommendations', 'No recommendations available')}
          
          ---
          *This report is automatically generated by the cross-model audit workflow.*
          """
                      
                      with open('reports/latest_audit.md', 'w') as f:
                          f.write(summary)
                      
                      print("Audit report generated successfully")
                  except Exception as e:
                      print(f"Error generating report: {e}")
              else:
                  print("No audit reports found")
          else:
              print("Reports directory does not exist")
          EOF

      - name: Update dashboard with audit results
        run: |
          echo "Cross-model audit completed. Results saved to reports/"

      - name: Commit audit results
        run: |
          git config --local user.email "github-actions[bot]@users.noreply.github.com"
          git config --local user.name "github-actions[bot]"
          git add reports/
          git diff --staged --quiet || git commit -m "chore: cross-model audit results $(date +%Y-%m-%d)"
          git push

      - name: Create issue for critical findings
        if: always()
        env:
          GITHUB_TOKEN: ${{ secrets.GITHUB_TOKEN }}
        run: |
          echo "Check audit results for any critical findings"
