name: Weekly Benchmark Archive

on:
  schedule:
    # Run every Sunday at 00:00 UTC
    - cron: '0 0 * * 0'
  workflow_dispatch:  # Allow manual triggering
  push:
    paths:
      - 'scripts/**'
      - 'src/utils/**'
      - '.github/workflows/weekly-benchmarks.yml'

# Ensure only one workflow runs at a time
concurrency:
  group: benchmark-${{ github.ref }}
  cancel-in-progress: false

# Default permissions (read-only)
permissions:
  contents: read

jobs:
  run-benchmarks:
    runs-on: ubuntu-latest
    
    # Job-specific permissions
    permissions:
      contents: write  # Required for committing and pushing benchmark results
      actions: read    # Required for workflow operations
    
    steps:
      - name: Checkout repository
        uses: actions/checkout@11bd71901bbe5b1630ceea73d27597364c9af683  # v4.2.2
        with:
          fetch-depth: 0
          submodules: true  # Fetch submodules including vendor/SWE-bench
      
      - name: Set up Python
        uses: actions/setup-python@0b93645e9fea7318ecaed2b359559ac225c90a2b  # v5.3.0
        with:
          python-version: '3.11'
          cache: 'pip'
          cache-dependency-path: |
            **/requirements*.txt
            **/setup.py
            **/pyproject.toml
      
      - name: Cache benchmark dependencies
        uses: actions/cache@1bd1e32a3bdc45362d1e726936510720a7c30a57  # v4.2.0
        with:
          path: |
            ~/.cache/pip
            ~/.cache/huggingface
          key: benchmark-${{ runner.os }}-${{ hashFiles('scripts/*.py') }}
          restore-keys: |
            benchmark-${{ runner.os }}-
      
      - name: Install dependencies
        run: |
          python -m pip install --upgrade pip
          # Install minimal dependencies for benchmarks
          # datasets library is optional - scripts will use simulation mode if not available
          if ! pip install datasets; then
            echo "Warning: datasets library could not be installed, will use simulation mode"
          fi
      
      - name: Run legacy benchmarks
        run: |
          python scripts/run_benchmarks.py --benchmark all
      
      - name: Run SWE-bench Verified
        run: |
          python scripts/swe_run.py --max-tasks 5 --num-workers 1
        continue-on-error: true  # Don't fail workflow if benchmark fails
      
      - name: Run GPQA Diamond
        run: |
          python scripts/gpqa_run.py --limit 100
        continue-on-error: true
      
      - name: Run KEGG KGML
        run: |
          python scripts/bio_kegg_run.py --max-pathways 10
        continue-on-error: true
      
      - name: Archive benchmark results
        uses: actions/upload-artifact@b4b15b8c7c6ac21ea08fcf65892d2ee8f75cf882  # v4.4.3
        with:
          name: benchmark-results-${{ github.run_number }}
          path: logs/benchmarks/*.json
          retention-days: 90
      
      - name: Commit results to repository
        run: |
          git config --local user.email "github-actions[bot]@users.noreply.github.com"
          git config --local user.name "github-actions[bot]"
          git add logs/benchmarks/*.json || true
          
          if git diff --staged --quiet; then
            echo "No changes to commit"
            echo "COMMIT_RESULT=no_changes" >> $GITHUB_ENV
          else
            if git commit -m "chore: weekly benchmark results $(date +%Y-%m-%d)"; then
              echo "Changes committed successfully"
              
              # Try to push with retry logic
              max_retries=3
              retry_count=0
              until git push || [ $retry_count -eq $max_retries ]; do
                retry_count=$((retry_count + 1))
                echo "Push attempt $retry_count failed, retrying in 5 seconds..."
                sleep 5
                git pull --rebase
              done
              
              if [ $retry_count -eq $max_retries ]; then
                echo "Failed to push after $max_retries attempts"
                echo "COMMIT_RESULT=push_failed" >> $GITHUB_ENV
                exit 1
              else
                echo "COMMIT_RESULT=success" >> $GITHUB_ENV
              fi
            else
              echo "Commit failed"
              echo "COMMIT_RESULT=commit_failed" >> $GITHUB_ENV
              exit 1
            fi
          fi
        env:
          GITHUB_TOKEN: ${{ secrets.GITHUB_TOKEN }}
      
      - name: Create benchmark summary
        if: always()
        run: |
          echo "## Weekly Benchmark Summary" >> $GITHUB_STEP_SUMMARY
          echo "" >> $GITHUB_STEP_SUMMARY
          echo "**Date**: $(date)" >> $GITHUB_STEP_SUMMARY
          echo "" >> $GITHUB_STEP_SUMMARY
          echo "**Commit Status**: ${COMMIT_RESULT:-unknown}" >> $GITHUB_STEP_SUMMARY
          echo "" >> $GITHUB_STEP_SUMMARY
          
          if [ "${COMMIT_RESULT}" = "success" ] || [ "${COMMIT_RESULT}" = "no_changes" ]; then
            echo "✅ Benchmark results have been generated and archived." >> $GITHUB_STEP_SUMMARY
            echo "" >> $GITHUB_STEP_SUMMARY
            echo "Results can be found in \`logs/benchmarks/\`" >> $GITHUB_STEP_SUMMARY
          else
            echo "⚠️ Benchmark results were generated but may not have been committed." >> $GITHUB_STEP_SUMMARY
            echo "" >> $GITHUB_STEP_SUMMARY
            echo "Check the workflow logs for details. Results are still available as artifacts." >> $GITHUB_STEP_SUMMARY
          fi
