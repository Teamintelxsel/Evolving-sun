name: Weekly Benchmark Archive

on:
  schedule:
    # Run every Sunday at 00:00 UTC
    - cron: '0 0 * * 0'
  workflow_dispatch:  # Allow manual triggering

jobs:
  run-benchmarks:
    runs-on: ubuntu-latest
    
    permissions:
      contents: write  # Required for committing and pushing benchmark results
      actions: read    # Required for workflow operations
    
    steps:
      - name: Checkout repository
        uses: actions/checkout@v4
        with:
          fetch-depth: 0
          submodules: 'recursive'  # Initialize submodules including SWE-bench
      
      - name: Set up Python
        uses: actions/setup-python@v5
        with:
          python-version: '3.11'
      
      - name: Install dependencies
        run: |
          python -m pip install --upgrade pip
          pip install -r requirements.txt || echo "No requirements.txt or install failed"
      
      - name: Run benchmarks
        run: |
          python scripts/run_benchmarks.py --benchmark all
      
      - name: Validate benchmark results
        id: validate
        run: |
          echo "Validating benchmark results..."
          python scripts/verify_benchmarks.py --all --fail-on-warning || {
            echo "VALIDATION_FAILED=true" >> $GITHUB_ENV
            echo "::warning::Benchmark validation found issues. Results will still be archived but not committed."
            exit 0  # Don't fail the workflow, just warn
          }
          echo "VALIDATION_FAILED=false" >> $GITHUB_ENV
          echo "✅ Benchmark validation passed"
      
      - name: Archive benchmark results
        uses: actions/upload-artifact@v4
        with:
          name: benchmark-results-${{ github.run_number }}
          path: logs/benchmarks/*.json
          retention-days: 90
      
      - name: Commit results to repository
        run: |
          # Only commit if validation passed
          if [ "${VALIDATION_FAILED}" = "true" ]; then
            echo "Skipping commit due to validation failures"
            echo "COMMIT_RESULT=validation_failed" >> $GITHUB_ENV
            exit 0
          fi
          
          git config --local user.email "github-actions[bot]@users.noreply.github.com"
          git config --local user.name "github-actions[bot]"
          git add logs/benchmarks/*.json || true
          
          if git diff --staged --quiet; then
            echo "No changes to commit"
            echo "COMMIT_RESULT=no_changes" >> $GITHUB_ENV
          else
            if git commit -m "chore: weekly benchmark results $(date +%Y-%m-%d)"; then
              echo "Changes committed successfully"
              
              # Try to push with retry logic
              max_retries=3
              retry_count=0
              until git push || [ $retry_count -eq $max_retries ]; do
                retry_count=$((retry_count + 1))
                echo "Push attempt $retry_count failed, retrying in 5 seconds..."
                sleep 5
                git pull --rebase
              done
              
              if [ $retry_count -eq $max_retries ]; then
                echo "Failed to push after $max_retries attempts"
                echo "COMMIT_RESULT=push_failed" >> $GITHUB_ENV
                exit 1
              else
                echo "COMMIT_RESULT=success" >> $GITHUB_ENV
              fi
            else
              echo "Commit failed"
              echo "COMMIT_RESULT=commit_failed" >> $GITHUB_ENV
              exit 1
            fi
          fi
        env:
          GITHUB_TOKEN: ${{ secrets.GITHUB_TOKEN }}
      
      - name: Create benchmark summary
        if: always()
        run: |
          echo "## Weekly Benchmark Summary" >> $GITHUB_STEP_SUMMARY
          echo "" >> $GITHUB_STEP_SUMMARY
          echo "**Date**: $(date)" >> $GITHUB_STEP_SUMMARY
          echo "" >> $GITHUB_STEP_SUMMARY
          echo "**Commit Status**: ${COMMIT_RESULT:-unknown}" >> $GITHUB_STEP_SUMMARY
          echo "" >> $GITHUB_STEP_SUMMARY
          
          if [ "${COMMIT_RESULT}" = "success" ] || [ "${COMMIT_RESULT}" = "no_changes" ]; then
            echo "✅ Benchmark results have been generated and archived." >> $GITHUB_STEP_SUMMARY
            echo "" >> $GITHUB_STEP_SUMMARY
            echo "Results can be found in \`logs/benchmarks/\`" >> $GITHUB_STEP_SUMMARY
          elif [ "${COMMIT_RESULT}" = "validation_failed" ]; then
            echo "⚠️ Benchmark results failed validation and were not committed." >> $GITHUB_STEP_SUMMARY
            echo "" >> $GITHUB_STEP_SUMMARY
            echo "Results are available as artifacts but contain issues that need to be resolved." >> $GITHUB_STEP_SUMMARY
            echo "" >> $GITHUB_STEP_SUMMARY
            echo "Common issues:" >> $GITHUB_STEP_SUMMARY
            echo "- Unrealistic execution times (< 1ms)" >> $GITHUB_STEP_SUMMARY
            echo "- Placeholder or simulated data" >> $GITHUB_STEP_SUMMARY
            echo "- Missing provenance information" >> $GITHUB_STEP_SUMMARY
          else
            echo "⚠️ Benchmark results were generated but may not have been committed." >> $GITHUB_STEP_SUMMARY
            echo "" >> $GITHUB_STEP_SUMMARY
            echo "Check the workflow logs for details. Results are still available as artifacts." >> $GITHUB_STEP_SUMMARY
          fi
