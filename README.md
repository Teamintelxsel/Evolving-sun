# Evolving-sun

Production-ready benchmark system with cryptographic verification, distributed execution, and real-time monitoring.

## Overview

This repository provides a comprehensive benchmarking infrastructure featuring:
- **ðŸš€ 500-Iteration Meta-Optimization**: Progressive accuracy targets (90% â†’ 95% â†’ 99.9%)
- **ðŸ”’ Cryptographic Verification**: SHA256 hashing and validation of all results
- **âš¡ Real Latency Measurement**: High-precision timing with nanosecond accuracy
- **ðŸŒ Distributed Execution**: Multi-GPU parallel processing via Ray
- **ðŸ“Š Real-Time Dashboard**: Web-based monitoring with live metrics
- **ðŸ“ˆ Statistical Analysis**: 99% confidence intervals and convergence detection
- **âœ… Comprehensive Validation**: Automated detection of simulated/fake data
- **ðŸ’¾ Checkpoint/Resume**: Save progress every 10 iterations

## Quick Start (3 Commands)

```bash
# 1. Install dependencies
pip install -r requirements.txt

# 2. Run quick test (10 iterations, ~1 minute)
bash examples/quick_start.sh

# 3. Start dashboard (optional)
python dashboard/app.py  # Open http://localhost:8080
```

## Architecture

```
Evolving-sun Benchmark System
â”‚
â”œâ”€â”€ Core Components
â”‚   â”œâ”€â”€ Meta-Optimization Engine (500 iterations â†’ 99.9% accuracy)
â”‚   â”œâ”€â”€ Latency Benchmarking (real measurements, not simulated)
â”‚   â”œâ”€â”€ Verification System (SHA256, schema validation)
â”‚   â””â”€â”€ CPU Stress Controller (99.9% utilization target)
â”‚
â”œâ”€â”€ Distributed Execution
â”‚   â”œâ”€â”€ Ray-based parallelization
â”‚   â”œâ”€â”€ Multi-GPU support (10+ workers)
â”‚   â””â”€â”€ Fault tolerance & retry logic
â”‚
â”œâ”€â”€ Monitoring & Reporting
â”‚   â”œâ”€â”€ Real-time web dashboard
â”‚   â”œâ”€â”€ API endpoints (status, metrics, benchmarks)
â”‚   â””â”€â”€ Progress tracking & notifications
â”‚
â””â”€â”€ Data Management
    â”œâ”€â”€ Checkpoint system (every 10 iterations)
    â”œâ”€â”€ JSON & CSV export
    â””â”€â”€ Watermarked results with provenance
```

## Repository Structure

```
Evolving-sun/
â”œâ”€â”€ .github/workflows/       # CI/CD workflows
â”œâ”€â”€ config/
â”‚   â”œâ”€â”€ benchmark_config.yaml    # Centralized configuration
â”‚   â””â”€â”€ notifications.yaml       # Notification settings
â”œâ”€â”€ dashboard/
â”‚   â”œâ”€â”€ app.py                   # Flask web dashboard
â”‚   â””â”€â”€ benchmark_data.db        # SQLite database
â”œâ”€â”€ docs/
â”‚   â”œâ”€â”€ LATENCY_OPTIMIZATION.md  # Optimization guide
â”‚   â”œâ”€â”€ DISTRIBUTED_EXECUTION.md # Multi-GPU setup
â”‚   â””â”€â”€ DASHBOARD_SETUP.md       # Dashboard deployment
â”œâ”€â”€ examples/
â”‚   â”œâ”€â”€ quick_start.sh           # One-command demo
â”‚   â””â”€â”€ verify_results.sh        # Validation tool
â”œâ”€â”€ logs/
â”‚   â”œâ”€â”€ benchmarks/              # Benchmark results (JSON/CSV)
â”‚   â””â”€â”€ checkpoints/             # Iteration checkpoints
â”œâ”€â”€ schemas/
â”‚   â””â”€â”€ benchmark_result.json    # JSON schema for validation
â”œâ”€â”€ scripts/
â”‚   â”œâ”€â”€ meta_optimization_500.py      # 500-iteration system
â”‚   â”œâ”€â”€ latency_benchmark_real.py     # Real latency testing
â”‚   â”œâ”€â”€ distributed_executor.py       # Multi-GPU execution
â”‚   â”œâ”€â”€ cpu_stress_controller.py      # CPU stress testing
â”‚   â”œâ”€â”€ verify_benchmarks.py          # Validation script
â”‚   â””â”€â”€ run_benchmarks.py             # Legacy unified runner
â””â”€â”€ requirements.txt             # Python dependencies
```

## Getting Started

### Prerequisites

- Python 3.9 or higher
- (Optional) NVIDIA GPUs for distributed execution
- (Optional) Ray for multi-GPU parallelization

### Installation

```bash
# Clone repository
git clone https://github.com/Teamintelxsel/Evolving-sun.git
cd Evolving-sun

# Install core dependencies
pip install -r requirements.txt

# (Optional) Install Ray for distributed execution
pip install 'ray[default]>=2.5.0'
```

## Usage

### Running Benchmarks

#### Quick Test (Dry Run)
```bash
# 10-iteration test (~1 minute)
python scripts/meta_optimization_500.py --dry-run
```

#### Full 500-Iteration Run
```bash
# Serial execution (~8 hours)
python scripts/meta_optimization_500.py

# Distributed execution with 10 GPUs (~50 minutes)
python scripts/distributed_executor.py --iterations 500 --workers 10
```

#### Latency Benchmarking
```bash
# Real latency measurement (1000 requests)
python scripts/latency_benchmark_real.py \
    --mode simulation \
    --requests 1000 \
    --workers 10

# HTTP endpoint testing
python scripts/latency_benchmark_real.py \
    --mode http \
    --url https://api.example.com/v1/predict \
    --requests 1000
```

#### Verifying Results
```bash
# Verify single file
python scripts/verify_benchmarks.py logs/benchmarks/meta_opt_*.json

# Verify entire directory
python scripts/verify_benchmarks.py logs/benchmarks/

# Save verification report
python scripts/verify_benchmarks.py logs/benchmarks/ \
    --output logs/verification_report.json
```

### Dashboard

```bash
# Start web dashboard
python dashboard/app.py

# Access at http://localhost:8080
# API endpoints available at /api/status, /api/benchmarks, /api/latest
```

### Checkpoint & Resume

```bash
# Checkpoints are saved automatically every 10 iterations
# Resume from checkpoint:
python scripts/meta_optimization_500.py \
    --resume checkpoints/iter_250.json
```

## Features

### Meta-Optimization System

- **Progressive Targets**: 
  - Phase 1 (Exploration, 1-100): 90% accuracy
  - Phase 2 (Refinement, 101-300): 95% accuracy  
  - Phase 3 (Convergence, 301-500): 99.9% accuracy
  
- **Convergence Detection**: Automatically stops when CV < 0.1%
- **Statistical Analysis**: 95% and 99% confidence intervals
- **Export Formats**: JSON (detailed) and CSV (analysis)

### Verification System

Detects fake/simulated data through:
- âœ… Timing validation (flags duration < 0.01s)
- âœ… SHA256 hash verification
- âœ… Statistical validity checks
- âœ… JSON schema compliance
- âœ… Hardware specification validation
- âœ… Percentile ordering verification

### Distributed Execution

- **Automatic GPU Detection**: Uses all available GPUs
- **Fault Tolerance**: Retries failed iterations
- **Progress Tracking**: Real-time status updates
- **Resource Monitoring**: Per-GPU utilization tracking
- **Graceful Shutdown**: Saves checkpoints on interrupt

### Performance Targets

| Mode | Duration | Workers | Expected Result |
|------|----------|---------|-----------------|
| Dry Run | ~1 min | 1 | 10 iterations complete |
| Serial | ~8 hours | 1 | 500 iterations, 99.9% accuracy |
| Distributed (10 GPUs) | ~50 min | 10 | 500 iterations, parallel |

## Configuration

### Benchmark Configuration

Edit `config/benchmark_config.yaml`:
```yaml
meta_optimization:
  total_iterations: 500
  checkpoint_interval: 10
  early_stopping: true

distributed:
  enabled: true
  num_workers: 10
  gpu_per_worker: 1

benchmark:
  cpu_target: 99.9
  latency:
    requests: 1000
    workers: 10
```

### Notification Configuration

Edit `config/notifications.yaml` and set environment variables:
```bash
export SLACK_WEBHOOK_URL="https://hooks.slack.com/..."
export EMAIL_USERNAME="your@email.com"
export EMAIL_PASSWORD="password"
```

## Contributing

Contributions are welcome! Please read [CONTRIBUTING.md](CONTRIBUTING.md) for details on our code of conduct and the process for submitting pull requests.

## Citation

If you use this benchmark system in your research, please cite:

```bibtex
@software{evolving_sun_benchmark,
  title = {Evolving-sun: Production-Ready Benchmark System},
  author = {Teamintelxsel},
  year = {2026},
  url = {https://github.com/Teamintelxsel/Evolving-sun},
  note = {Cryptographically verified benchmark system with distributed execution}
}
```

## License

This project is open source and available under the MIT License.

## Support

- **Issues**: [GitHub Issues](https://github.com/Teamintelxsel/Evolving-sun/issues)
- **Documentation**: See `docs/` directory
- **Examples**: See `examples/` directory

## Acknowledgments

- Built with Flask, Ray, NumPy, SciPy
- Inspired by production ML benchmarking best practices
- Verification methodology based on cryptographic standards

## Documentation

- **[Latency Optimization Guide](docs/LATENCY_OPTIMIZATION.md)** - 10+ optimization techniques with expected gains
- **[Distributed Execution Guide](docs/DISTRIBUTED_EXECUTION.md)** - Multi-GPU setup and configuration
- **[Dashboard Setup Guide](docs/DASHBOARD_SETUP.md)** - Web dashboard deployment
- **[Examples](examples/)** - Quick start and verification scripts
- **[Benchmark Documentation](scripts/BENCHMARKS.md)** - Legacy benchmark harness integrations

## Example Results

### Meta-Optimization (Dry Run)

```
======================================================================
META-OPTIMIZATION 500 RESULTS
======================================================================

Mode: DRY RUN (10 iterations)
Completed: 10/10
Duration: 0.50s

Final Accuracy: 0.8985 (89.85%)
Final Latency: 68.00ms
Final Throughput: 108.00 req/s

Accuracy Statistics:
  Mean: 0.8761
  Std Dev: 0.0241
  CV: 0.0275
  95% CI: [0.8571, 0.8951]
  99% CI: [0.8509, 0.9013]
  Converged: âœ“

Verification: True
SHA256: 5b4a68a377e5df5e...
======================================================================
```

### Verification Report

```
======================================================================
BENCHMARK VERIFICATION REPORT
======================================================================

Total files: 3
Passed: 1 (33.3%)
Failed: 2 (66.7%)

Individual Results:
  âœ“ PASS: meta_opt_20260112.json
    - All checks passed
    - Duration: 1.01s (realistic)
    - Verification hash valid
  
  âœ— FAIL: benchmark_old_simulated.json
    - Error: Suspiciously fast execution (0.000016s)
    - Error: Missing verification block
    - Warning: No hardware specifications
======================================================================
```

## Roadmap

### Completed âœ…
- [x] Core benchmark infrastructure (latency, CPU stress, verification)
- [x] Meta-optimization with 500-iteration support
- [x] JSON schema validation
- [x] Cryptographic verification (SHA256)
- [x] Checkpoint/resume capability
- [x] Statistical convergence detection
- [x] CSV export for analysis
- [x] Basic web dashboard
- [x] Distributed executor framework
- [x] Comprehensive documentation
- [x] Example scripts and quick start

### In Progress ðŸš§
- [ ] Full WebSocket-based real-time dashboard
- [ ] Multi-channel notification system (Slack, Email, Discord)
- [ ] Automated leaderboard submission
- [ ] Enhanced distributed execution with Ray
- [ ] Docker containerization
- [ ] CI/CD integration for automated validation

### Planned ðŸ“‹
- [ ] Agent arena for multi-agent competition
- [ ] Optimization rewrite engine (INT8, Flash Attention, etc.)
- [ ] GPU temperature and power monitoring
- [ ] Prometheus metrics export
- [ ] Grafana dashboard templates
- [ ] Weekly scheduled 500-iteration runs
